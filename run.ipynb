{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import h5py\n",
    "import io\n",
    "import json\n",
    "import JsonUtil\n",
    "import tensorflow as tf\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Merge, Dropout, RepeatVector, merge, Flatten\n",
    "from keras.layers import recurrent, Input, Bidirectional, LSTM, Lambda, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import optimizers\n",
    "## from rnnlayer import Attention, SimpleAttention\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the glove dictionary into the memory for usage in \n",
    "## making the word embeddings\n",
    "GloveMatrix = {}\n",
    "dimensionIndex = '300'\n",
    "validationContext = [] \n",
    "validationQuestion = [] \n",
    "validationQuestion_id = [] \n",
    "validationToken2CharIdx = []\n",
    "validationContextOriginal = []\n",
    "maxLenvalidationContext = 0\n",
    "maxLenvalidationQuestion = 0\n",
    "trainContext = [] \n",
    "trainQuestion = [] \n",
    "trainQuestion_id = [] \n",
    "trainAnswerBegin = [] \n",
    "trainAnswerEnd = [] \n",
    "trainAnswerText = [] \n",
    "maxLenTrainingContext = 0\n",
    "maxLenTrainingQuestion = 0\n",
    "vocabulary = {}\n",
    "contextMaxlen = 0\n",
    "questionMax = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for loading the glove files into the memory it\n",
    "## will be used for vectorizing the vocabulary that we will be\n",
    "## creating\n",
    "def getGloveDictionary(gloveFile):\n",
    "    f = open(gloveFile,'r',encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        currWord = values[0]\n",
    "        vectorizedNotation = values[1:]\n",
    "        ## converting vectorized notation to array so that we \n",
    "        ## can directly use this as an arrya\n",
    "        vector = np.asarray(vectorizedNotation, dtype='float32')\n",
    "        GloveMatrix[currWord] = vector \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizing the line\n",
    "## returning the tokenized line with the tokens\n",
    "def tokenize(currLine):\n",
    "    finalTokens = []\n",
    "    tokens = re.split('(\\W+)?', currLine)\n",
    "    for currToken in tokens:\n",
    "        if (currToken.strip()):\n",
    "            finalTokens.append(currToken.strip())\n",
    "    return finalTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeCommonData(data):\n",
    "    data1 = data.replace(\"''\", '\" ')\n",
    "    data1 = data1.replace(\"``\", '\" ')\n",
    "    data = data.lower()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTrainingQues(qas, context, contextTokenized):\n",
    "    global maxLenTrainingQuestion\n",
    "    for qa in qas:\n",
    "        question = qa._question_\n",
    "        question = removeCommonData(question)\n",
    "        questionTokenized = tokenize(question)\n",
    "        if len(questionTokenized) > maxLenTrainingQuestion:\n",
    "            maxLenTrainingQuestion = len(questionTokenized)\n",
    "        question_id = qa._id_\n",
    "        answers = qa._answers_\n",
    "        for answer in answers:\n",
    "            answerText = answer._text_\n",
    "            answerTokenized = tokenize(answerText.lower())\n",
    "            # find indices of beginning/ending words of answer span among tokenized context\n",
    "            contextToAnswerFirstWord = context[:answer._answer_start_ + len(answerTokenized[0])]\n",
    "            answerBeginIndex = len(tokenize(contextToAnswerFirstWord.lower())) - 1\n",
    "            answerEndIndex = answerBeginIndex + len(answerTokenized) - 1\n",
    "            trainQuestion.append(questionTokenized)\n",
    "            trainQuestion_id.append(str(question_id))\n",
    "            trainAnswerBegin.append(answerBeginIndex)\n",
    "            trainAnswerEnd.append(answerEndIndex)\n",
    "            trainAnswerText.append(answerText)\n",
    "            trainContext.append(contextTokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDatasets(f):\n",
    "    global maxLenTrainingContext\n",
    "    for data in f:\n",
    "        paragraphs = data._paragraphs_\n",
    "        for paragraph in paragraphs:\n",
    "            context = paragraph._context_\n",
    "            context1 = context.replace(\"''\", '\" ')\n",
    "            context1 = context1.replace(\"``\", '\" ')\n",
    "            contextTokenized = tokenize(context.lower())\n",
    "            contextLength = len(contextTokenized)\n",
    "            if contextLength > maxLenTrainingContext:\n",
    "                maxLenTrainingContext = contextLength\n",
    "            qas = paragraph._qas_\n",
    "            processTrainingQues(qas, context1, contextTokenized);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(line):\n",
    "    #tokens1 = nltk.word_tokenize(line)\n",
    "    tokens = tokenize(line)\n",
    "    ## removing bad datasets from the tokens\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].replace(\"''\", '\"')\n",
    "        tokens[i].replace(\"``\", '\"')\n",
    "    idx = 0;\n",
    "    token_idx = 0;\n",
    "    tokensStartIndex = [None]*len(tokens)   \n",
    "    '''Code for getting the respective tokens and there respective index'''\n",
    "    while idx < len(line) and token_idx < len(tokens):\n",
    "        word = tokens[token_idx]\n",
    "        if line[idx:idx+len(word)] == word:\n",
    "            tokensStartIndex[token_idx] = idx\n",
    "            idx = idx + len(word)\n",
    "            token_idx = token_idx + 1 \n",
    "        else:\n",
    "            idx = idx + 1\n",
    "    return tokens, tokensStartIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processValidationQuestions(qas, tokenIdx2CharIdx, context, contextTokenized):\n",
    "    global maxLenvalidationQuestion\n",
    "    for qa in qas:\n",
    "        question = qa._question_\n",
    "        question = removeCommonData(question);\n",
    "        questionTokenized = tokenize(question)\n",
    "        if len(questionTokenized) > maxLenvalidationQuestion:\n",
    "            maxLenvalidationQuestion = len(questionTokenized)\n",
    "        question_id = qa._id_\n",
    "        answers = qa._answers_\n",
    "        validationQuestion.append(questionTokenized)\n",
    "        validationQuestion_id.append(str(question_id))\n",
    "        validationToken2CharIdx.append(tokenIdx2CharIdx)\n",
    "        validationContextOriginal.append(context)\n",
    "        validationContext.append(contextTokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitValDatasets(f): \n",
    "    global maxLenvalidationContext\n",
    "    for data in f:\n",
    "        paragraphs = data._paragraphs_\n",
    "        for paragraph in paragraphs:\n",
    "            context = paragraph._context_\n",
    "            context1 = context.replace(\"''\", '\" ')\n",
    "            context1 = context1.replace(\"``\", '\" ')\n",
    "            contextTokenized, tokenIdx2CharIdx = getTokens(context1.lower())\n",
    "            contextLength = len(contextTokenized)\n",
    "            if contextLength > maxLenvalidationContext:\n",
    "                maxLenvalidationContext = contextLength\n",
    "            qas = paragraph._qas_\n",
    "            processValidationQuestions(qas, tokenIdx2CharIdx, context, contextTokenized);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeTrainingData(dictionaryWordIndex, maximumcontext, maximumquestion):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    YBegin = []\n",
    "    YEnd = []\n",
    "    for i in range(len(trainContext)):\n",
    "        x=[]\n",
    "        xq=[]\n",
    "        for word in trainContext[i]:\n",
    "            x.append(dictionaryWordIndex[word])\n",
    "        for word in trainQuestion[i]:\n",
    "            xq.append(dictionaryWordIndex[word])\n",
    "        # map the first and last words of answer span to one-hot representations\n",
    "        y_Begin =  np.zeros(len(trainContext[i]))\n",
    "        y_Begin[trainAnswerBegin[i]] = 1\n",
    "        y_End = np.zeros(len(trainContext[i]))\n",
    "        y_End[trainAnswerEnd[i]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        YBegin.append(y_Begin)\n",
    "        YEnd.append(y_End)\n",
    "    return pad_sequences(X, maxlen=maximumcontext, padding='post'),pad_sequences(Xq, maxlen=maximumquestion, padding='post'),pad_sequences(YBegin, maxlen=maximumcontext, padding='post'),pad_sequences(YEnd, maxlen=maximumcontext, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeValData(dictionaryWordIndex, maximumcontext, maximumquestion):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    YBegin = []\n",
    "    YEnd = []\n",
    "    for i in range(len(validationContext)):\n",
    "        x=[]\n",
    "        xq=[]\n",
    "        for word in validationContext[i]:\n",
    "            x.append(dictionaryWordIndex[word])\n",
    "        for word in validationQuestion[i]:\n",
    "            xq.append(dictionaryWordIndex[word])\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "    return pad_sequences(X, maxlen=maximumcontext, padding='post'), pad_sequences(Xq, maxlen=maximumquestion, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    trainData = JsonUtil.import_qas_data('data/train-v1.1.json')\n",
    "    splitDatasets(trainData)\n",
    "    valData = JsonUtil.import_qas_data('data/dev-v1.1.json')\n",
    "    splitValDatasets(valData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Change the below function correctly'''\n",
    "def getVocab():\n",
    "    vocabulary = {}\n",
    "    global maxLenTContext\n",
    "    global maxLenVContext\n",
    "    for words in trainContext+trainQuestion+validationContext+validationQuestion:\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = 1\n",
    "#     for words in trainQuestion:\n",
    "#         for word in words:\n",
    "#             if word in vocabulary:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 vocabulary[word] = 1\n",
    "#     for words in validationContext:\n",
    "#         for word in words:\n",
    "#             if word in vocabulary:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 vocabulary[word] = 1\n",
    "#     for words in validationQuestion:\n",
    "#         for word in words:\n",
    "#             if word not in vocabulary:\n",
    "#                 vocabulary[word] = 1\n",
    "    vocabulary = sorted(vocabulary.keys())  \n",
    "#     vocab_size = len(vocabulary) + 1\n",
    "    dictionaryWordIndex = {}\n",
    "    for i in range(len(vocabulary)):\n",
    "        dictionaryWordIndex[vocabulary[i]]=i+1\n",
    "    maximumcontext = 0\n",
    "    maximumquestion = 0\n",
    "    if (maxLenTrainingContext > maxLenvalidationContext):\n",
    "        maximumcontext = maxLenTrainingContext\n",
    "    else:\n",
    "        maximumcontext = maxLenvalidationContext    \n",
    "    if(maxLenvalidationQuestion > maxLenTrainingQuestion):\n",
    "        maximumquestion = maxLenvalidationQuestion\n",
    "    else:\n",
    "        maximumquestion = maxLenTrainingQuestion\n",
    "    return vocabulary, dictionaryWordIndex, maximumcontext, maximumquestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectorizedData(vocabulary, dictionaryWordIndex, maximumcontext, maximumquestion):    \n",
    "    tX, tXq, tYBegin, tYEnd = vectorizeTrainingData(dictionaryWordIndex, maximumcontext, maximumquestion)\n",
    "    randindex = np.random.permutation(tX.shape[0])\n",
    "    tX = tX[randindex, :]\n",
    "    tXq = tXq[randindex, :]\n",
    "    tYBegin = tYBegin[randindex, :]\n",
    "    tYEnd = tYEnd[randindex, :]\n",
    "    vX, vXq = vectorizeValData(dictionaryWordIndex, maximumcontext, maximumquestion)\n",
    "    return tX, tXq, tYBegin, tYEnd ,vX, vXq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEmbeddingMatrix(dictionary):\n",
    "    \n",
    "    EmbedMatrix = np.zeros((len(dictionary), int(dimensionIndex)))\n",
    "    for word, i in dictionary.items():\n",
    "        EmbedV = GloveMatrix.get(word)\n",
    "        if EmbedV is not None:\n",
    "            EmbedMatrix[i] = EmbedV\n",
    "    return EmbedMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    glovePath = 'data/glove.6B/glove.6B.' + str(dimensionIndex) + 'd.txt';\n",
    "    getGloveDictionary(glovePath);\n",
    "    loadData()\n",
    "    vocabulary, dictionaryWordIndex, maximumcontext, maximumquestion = getVocab()\n",
    "    tX, tXq, tYBegin, tYEnd ,vX, vXq = getVectorizedData(vocabulary, dictionaryWordIndex, maximumcontext, maximumquestion)\n",
    "    EmbedMatrix = createEmbeddingMatrix(dictionaryWordIndex)\n",
    "    embedding_layer = Embedding(len(dictionaryWordIndex),\n",
    "                            int(dimensionIndex),\n",
    "                            weights=[EmbedMatrix],\n",
    "                            input_length=maximumcontext,\n",
    "                            trainable=True)\n",
    "    print(\"maximumcontext\",maximumcontext)\n",
    "    print(\"maximumquestion\",maximumquestion)\n",
    "    cinput = Input(shape=(maximumcontext,), dtype='int32', name='cinput')\n",
    "    cembedding=embedding_layer(cinput)\n",
    "    qinput = Input(shape=(maximumquestion,), dtype='int32', name='qinput')\n",
    "    qembedding=embedding_layer(qinput)\n",
    "    # BiDirectional LSTM Layer\n",
    "    D = Bidirectional(LSTM(150, return_sequences=False))(cembedding)\n",
    "    # print(D.shape)\n",
    "    Q = Bidirectional(LSTM(150, return_sequences=False))(qembedding)\n",
    "    # print(Q.shape)\n",
    "    L = merge([D, Q], mode='concat')\n",
    "    ## Code for the Attention Layer \n",
    "    # print(qembedding.shape)\n",
    "    # print(qembedding.shape)\n",
    "    # Q = Bidirectional(LSTM(100, return_sequences=True))(qembedding)\n",
    "    # Q1 = Dropout(0.2)(Q)\n",
    "    # print(Q.shape)\n",
    "    # clstm1 = Attention(100, Q1, 200, return_sequences=True, name='clstm1')(cembedding)\n",
    "    # qlstm2 = Attention(100, clstm1, 100, return_sequences=True, name='qlstm2')(Q1)\n",
    "    # clstm2 = Attention(100, qlstm2, 100, return_sequences=True, name='clstm2')(clstm1)\n",
    "    # qlstm4 = Bidirectional(LSTM(100, name='qlstm4'))(qlstm3)\n",
    "    # h = merge([qlstm4, clstm3], mode='concat', name='merge1'\n",
    "    answerPtrBegin_output = Dense(maximumcontext, activation='softmax')(L)\n",
    "    Lmerge = merge([L, answerPtrBegin_output], mode='concat', name='merge2')\n",
    "    answerPtrEnd_output = Dense(maximumcontext, activation='softmax')(Lmerge)\n",
    "    model = Model(input=[cinput, qinput], output=[answerPtrBegin_output, answerPtrEnd_output])\n",
    "    adam = optimizers.Adam()#, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy',\n",
    "                  loss_weights=[.04, 0.04], metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    filepath1=\"weights/weights-matchlstm-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "    checkpoint1 = ModelCheckpoint(filepath1, monitor='val_loss', verbose=1, save_best_only=False, mode='auto')\n",
    "#     checkpoint2 = EarlyStopping(monitor='val_dense_1_loss', min_delta=0.01, patience=3, verbose=0, mode='auto')\n",
    "    callbacks_list = [checkpoint1] #checkpoint2 \n",
    "    model.fit([tX, tXq], [tYBegin, tYEnd], epochs=10, batch_size=128, shuffle=True, validation_split=0.25, \\\n",
    "              callbacks=callbacks_list)\n",
    "    predictions = model.predict([vX, vXq], batch_size=128)\n",
    "    ansBegin = np.zeros((predictions[0].shape[0],), dtype=np.int32)\n",
    "    ansEnd = np.zeros((predictions[0].shape[0],),dtype=np.int32) \n",
    "    for i in range(predictions[0].shape[0]):\n",
    "        ansBegin[i] = predictions[0][i, :].argmax()\n",
    "        ansEnd[i] = predictions[1][i, :].argmax()\n",
    "    answers = {}\n",
    "    for i in range(len(validationQuestion_id)):\n",
    "        #print i\n",
    "        if ansBegin[i] >= len(validationContext[i]):\n",
    "            answers[validationQuestion_id[i]] = \"\"\n",
    "        elif ansEnd[i] >= len(validationContext[i]):\n",
    "            answers[validationQuestion_id[i]] = validationContextOriginal[i][validationToken2CharIdx[i][ansBegin[i]]:]\n",
    "        else:\n",
    "            answers[validationQuestion_id[i]] = validationContextOriginal[i][validationToken2CharIdx[i][ansBegin[i]]:validationToken2CharIdx[i][ansEnd[i]]+len(validationContext[i][ansEnd[i]])]\n",
    "    with io.open('predictedanswer.json', 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(answers, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
