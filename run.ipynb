{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import h5py\n",
    "import io\n",
    "import json\n",
    "import JsonUtil\n",
    "import tensorflow as tf\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Merge, Dropout, RepeatVector, merge, Flatten\n",
    "from keras.layers import recurrent, Input, Bidirectional, LSTM, Lambda, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import optimizers\n",
    "## from rnnlayer import Attention, SimpleAttention\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the glove dictionary into the memory for usage in \n",
    "## making the word embeddings\n",
    "GloveMatrix = {}\n",
    "dimensionIndex = '300'\n",
    "validationContext = [] \n",
    "validationQuestion = [] \n",
    "validationQuestion_id = [] \n",
    "validationToken2CharIdx = []\n",
    "validationContextOriginal = []\n",
    "maxLenvalidationContext = 0\n",
    "maxLenvalidationQuestion = 0\n",
    "trainContext = [] \n",
    "trainQuestion = [] \n",
    "trainQuestion_id = [] \n",
    "trainAnswerBegin = [] \n",
    "trainAnswerEnd = [] \n",
    "trainAnswerText = [] \n",
    "maxLenTrainingContext = 0\n",
    "maxLenTrainingQuestion = 0\n",
    "vocabulary = {}\n",
    "contextMaxlen = 0\n",
    "questionMax = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for loading the glove files into the memory it\n",
    "## will be used for vectorizing the vocabulary that we will be\n",
    "## creating\n",
    "def getGloveDictionary(gloveFile):\n",
    "    f = open(gloveFile,'r',encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        currWord = values[0]\n",
    "        vectorizedNotation = values[1:]\n",
    "        ## converting vectorized notation to array so that we \n",
    "        ## can directly use this as an arrya\n",
    "        vector = np.asarray(vectorizedNotation, dtype='float32')\n",
    "        GloveMatrix[currWord] = vector \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizing the line\n",
    "## returning the tokenized line with the tokens\n",
    "def tokenize(currLine):\n",
    "    finalTokens = []\n",
    "    tokens = re.split('(\\W+)?', currLine)\n",
    "    for currToken in tokens:\n",
    "        if (currToken.strip()):\n",
    "            finalTokens.append(currToken.strip())\n",
    "    return finalTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeCommonData(data):\n",
    "    data1 = data.replace(\"''\", '\" ')\n",
    "    data1 = data1.replace(\"``\", '\" ')\n",
    "    data = data.lower()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTrainingQues(qas, context, contextTokenized):\n",
    "    global maxLenTrainingQuestion\n",
    "    for qa in qas:\n",
    "        question = qa._question_\n",
    "        question = removeCommonData(question)\n",
    "        questionTokenized = tokenize(question)\n",
    "        if len(questionTokenized) > maxLenTrainingQuestion:\n",
    "            maxLenTrainingQuestion = len(questionTokenized)\n",
    "        question_id = qa._id_\n",
    "        answers = qa._answers_\n",
    "        for answer in answers:\n",
    "            answerText = answer._text_\n",
    "            answerTokenized = tokenize(answerText.lower())\n",
    "            # find indices of beginning/ending words of answer span among tokenized context\n",
    "            contextToAnswerFirstWord = context[:answer._answer_start_ + len(answerTokenized[0])]\n",
    "            answerBeginIndex = len(tokenize(contextToAnswerFirstWord.lower())) - 1\n",
    "            answerEndIndex = answerBeginIndex + len(answerTokenized) - 1\n",
    "            trainQuestion.append(questionTokenized)\n",
    "            trainQuestion_id.append(str(question_id))\n",
    "            trainAnswerBegin.append(answerBeginIndex)\n",
    "            trainAnswerEnd.append(answerEndIndex)\n",
    "            trainAnswerText.append(answerText)\n",
    "            trainContext.append(contextTokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDatasets(f):\n",
    "    global maxLenTrainingContext\n",
    "    for data in f:\n",
    "        paragraphs = data._paragraphs_\n",
    "        for paragraph in paragraphs:\n",
    "            context = paragraph._context_\n",
    "            context1 = context.replace(\"''\", '\" ')\n",
    "            context1 = context1.replace(\"``\", '\" ')\n",
    "            contextTokenized = tokenize(context.lower())\n",
    "            contextLength = len(contextTokenized)\n",
    "            if contextLength > maxLenTrainingContext:\n",
    "                maxLenTrainingContext = contextLength\n",
    "            qas = paragraph._qas_\n",
    "            processTrainingQues(qas, context1, contextTokenized);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(line):\n",
    "    #tokens1 = nltk.word_tokenize(line)\n",
    "    tokens = tokenize(line)\n",
    "    ## removing bad datasets from the tokens\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].replace(\"''\", '\"')\n",
    "        tokens[i].replace(\"``\", '\"')\n",
    "    idx = 0;\n",
    "    token_idx = 0;\n",
    "    tokensStartIndex = [None]*len(tokens)   \n",
    "    '''Code for getting the respective tokens and there respective index'''\n",
    "    while idx < len(line) and token_idx < len(tokens):\n",
    "        word = tokens[token_idx]\n",
    "        if line[idx:idx+len(word)] == word:\n",
    "            tokensStartIndex[token_idx] = idx\n",
    "            idx = idx + len(word)\n",
    "            token_idx = token_idx + 1 \n",
    "        else:\n",
    "            idx = idx + 1\n",
    "    return tokens, tokensStartIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processValidationQuestions(qas, tokenIdx2CharIdx, context, contextTokenized):\n",
    "    global maxLenvalidationQuestion\n",
    "    for qa in qas:\n",
    "        question = qa._question_\n",
    "        question = removeCommonData(question);\n",
    "        questionTokenized = tokenize(question)\n",
    "        if len(questionTokenized) > maxLenvalidationQuestion:\n",
    "            maxLenvalidationQuestion = len(questionTokenized)\n",
    "        question_id = qa._id_\n",
    "        answers = qa._answers_\n",
    "        validationQuestion.append(questionTokenized)\n",
    "        validationQuestion_id.append(str(question_id))\n",
    "        validationToken2CharIdx.append(tokenIdx2CharIdx)\n",
    "        validationContextOriginal.append(context)\n",
    "        validationContext.append(contextTokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitValDatasets(f): \n",
    "    global maxLenvalidationContext\n",
    "    for data in f:\n",
    "        paragraphs = data._paragraphs_\n",
    "        for paragraph in paragraphs:\n",
    "            context = paragraph._context_\n",
    "            context1 = context.replace(\"''\", '\" ')\n",
    "            context1 = context1.replace(\"``\", '\" ')\n",
    "            contextTokenized, tokenIdx2CharIdx = getTokens(context1.lower())\n",
    "            contextLength = len(contextTokenized)\n",
    "            if contextLength > maxLenvalidationContext:\n",
    "                maxLenvalidationContext = contextLength\n",
    "            qas = paragraph._qas_\n",
    "            processValidationQuestions(qas, tokenIdx2CharIdx, context, contextTokenized);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeTrainingData(dictionaryWordIndex, maximumcontext, maximumquestion):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    YBegin = []\n",
    "    YEnd = []\n",
    "    for i in range(len(trainContext)):\n",
    "        x=[]\n",
    "        xq=[]\n",
    "        for word in trainContext[i]:\n",
    "            x.append(dictionaryWordIndex[word])\n",
    "        for word in trainQuestion[i]:\n",
    "            xq.append(dictionaryWordIndex[word])\n",
    "        # map the first and last words of answer span to one-hot representations\n",
    "        y_Begin =  np.zeros(len(trainContext[i]))\n",
    "        y_Begin[trainAnswerBegin[i]] = 1\n",
    "        y_End = np.zeros(len(trainContext[i]))\n",
    "        y_End[trainAnswerEnd[i]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        YBegin.append(y_Begin)\n",
    "        YEnd.append(y_End)\n",
    "    return pad_sequences(X, maxlen=maximumcontext, padding='post'),pad_sequences(Xq, maxlen=maximumquestion, padding='post'),pad_sequences(YBegin, maxlen=maximumcontext, padding='post'),pad_sequences(YEnd, maxlen=maximumcontext, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeValData(dictionaryWordIndex, maximumcontext, maximumquestion):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    YBegin = []\n",
    "    YEnd = []\n",
    "    for i in range(len(validationContext)):\n",
    "        x=[]\n",
    "        xq=[]\n",
    "        for word in validationContext[i]:\n",
    "            x.append(dictionaryWordIndex[word])\n",
    "        for word in validationQuestion[i]:\n",
    "            xq.append(dictionaryWordIndex[word])\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "    return pad_sequences(X, maxlen=maximumcontext, padding='post'), pad_sequences(Xq, maxlen=maximumquestion, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    trainData = JsonUtil.import_qas_data('data/train-v1.1.json')\n",
    "    splitDatasets(trainData)\n",
    "    valData = JsonUtil.import_qas_data('data/dev-v1.1.json')\n",
    "    splitValDatasets(valData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Change the below function correctly'''\n",
    "def getVocab():\n",
    "    vocabulary = {}\n",
    "    global maxLenTContext\n",
    "    global maxLenVContext\n",
    "    for words in trainContext+trainQuestion+validationContext+validationQuestion:\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = 1\n",
    "#     for words in trainQuestion:\n",
    "#         for word in words:\n",
    "#             if word in vocabulary:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 vocabulary[word] = 1\n",
    "#     for words in validationContext:\n",
    "#         for word in words:\n",
    "#             if word in vocabulary:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 vocabulary[word] = 1\n",
    "#     for words in validationQuestion:\n",
    "#         for word in words:\n",
    "#             if word not in vocabulary:\n",
    "#                 vocabulary[word] = 1\n",
    "    vocabulary = sorted(vocabulary.keys())  \n",
    "#     vocab_size = len(vocabulary) + 1\n",
    "    dictionaryWordIndex = {}\n",
    "    for i in range(len(vocabulary)):\n",
    "        dictionaryWordIndex[vocabulary[i]]=i+1\n",
    "    maximumcontext = 0\n",
    "    maximumquestion = 0\n",
    "    if (maxLenTrainingContext > maxLenvalidationContext):\n",
    "        maximumcontext = maxLenTrainingContext\n",
    "    else:\n",
    "        maximumcontext = maxLenvalidationContext    \n",
    "    if(maxLenvalidationQuestion > maxLenTrainingQuestion):\n",
    "        maximumquestion = maxLenvalidationQuestion\n",
    "    else:\n",
    "        maximumquestion = maxLenTrainingQuestion\n",
    "    return vocabulary, dictionaryWordIndex, maximumcontext, maximumquestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectorizedData(vocabulary, dictionaryWordIndex, maximumcontext, maximumquestion):    \n",
    "    tX, tXq, tYBegin, tYEnd = vectorizeTrainingData(dictionaryWordIndex, maximumcontext, maximumquestion)\n",
    "    randindex = np.random.permutation(tX.shape[0])\n",
    "    tX = tX[randindex, :]\n",
    "    tXq = tXq[randindex, :]\n",
    "    tYBegin = tYBegin[randindex, :]\n",
    "    tYEnd = tYEnd[randindex, :]\n",
    "    vX, vXq = vectorizeValData(dictionaryWordIndex, maximumcontext, maximumquestion)\n",
    "    return tX, tXq, tYBegin, tYEnd ,vX, vXq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEmbeddingMatrix(dictionary):\n",
    "    \n",
    "    EmbedMatrix = np.zeros((len(dictionary), int(dimensionIndex)))\n",
    "    for word, i in dictionary.items():\n",
    "        EmbedV = GloveMatrix.get(word)\n",
    "        if EmbedV is not None:\n",
    "            EmbedMatrix[i] = EmbedV\n",
    "    return EmbedMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    glovePath = 'data/glove.6B/glove.6B.' + str(dimensionIndex) + 'd.txt';\n",
    "    getGloveDictionary(glovePath);\n",
    "    loadData()\n",
    "    vocabulary, dictionaryWordIndex, maximumcontext, maximumquestion = getVocab()\n",
    "    tX, tXq, tYBegin, tYEnd ,vX, vXq = getVectorizedData(vocabulary, dictionaryWordIndex, maximumcontext, maximumquestion)\n",
    "    EmbedMatrix = createEmbeddingMatrix(dictionaryWordIndex)\n",
    "    embedding_layer = Embedding(len(dictionaryWordIndex),\n",
    "                            int(dimensionIndex),\n",
    "                            weights=[EmbedMatrix],\n",
    "                            input_length=maximumcontext,\n",
    "                            trainable=True)\n",
    "    print(\"maximumcontext\",maximumcontext)\n",
    "    print(\"maximumquestion\",maximumquestion)\n",
    "    cinput = Input(shape=(maximumcontext,), dtype='int32', name='cinput')\n",
    "    cembedding=embedding_layer(cinput)\n",
    "    qinput = Input(shape=(maximumquestion,), dtype='int32', name='qinput')\n",
    "    qembedding=embedding_layer(qinput)\n",
    "    # BiDirectional LSTM Layer\n",
    "    D = Bidirectional(LSTM(150, return_sequences=False))(cembedding)\n",
    "    # print(D.shape)\n",
    "    Q = Bidirectional(LSTM(150, return_sequences=False))(qembedding)\n",
    "    # print(Q.shape)\n",
    "    L = merge([D, Q], mode='concat')\n",
    "    ## Code for the Attention Layer \n",
    "    # print(qembedding.shape)\n",
    "    # print(qembedding.shape)\n",
    "    # Q = Bidirectional(LSTM(100, return_sequences=True))(qembedding)\n",
    "    # Q1 = Dropout(0.2)(Q)\n",
    "    # print(Q.shape)\n",
    "    # clstm1 = Attention(100, Q1, 200, return_sequences=True, name='clstm1')(cembedding)\n",
    "    # qlstm2 = Attention(100, clstm1, 100, return_sequences=True, name='qlstm2')(Q1)\n",
    "    # clstm2 = Attention(100, qlstm2, 100, return_sequences=True, name='clstm2')(clstm1)\n",
    "    # qlstm4 = Bidirectional(LSTM(100, name='qlstm4'))(qlstm3)\n",
    "    # h = merge([qlstm4, clstm3], mode='concat', name='merge1'\n",
    "    answerPtrBegin_output = Dense(maximumcontext, activation='softmax')(L)\n",
    "    Lmerge = merge([L, answerPtrBegin_output], mode='concat', name='merge2')\n",
    "    answerPtrEnd_output = Dense(maximumcontext, activation='softmax')(Lmerge)\n",
    "    model = Model(input=[cinput, qinput], output=[answerPtrBegin_output, answerPtrEnd_output])\n",
    "    adam = optimizers.Adam()#, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy',\n",
    "                  loss_weights=[.04, 0.04], metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    filepath1=\"weights/weights-matchlstm-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "    checkpoint1 = ModelCheckpoint(filepath1, monitor='val_loss', verbose=1, save_best_only=False, mode='auto')\n",
    "#     checkpoint2 = EarlyStopping(monitor='val_dense_1_loss', min_delta=0.01, patience=3, verbose=0, mode='auto')\n",
    "    callbacks_list = [checkpoint1] #checkpoint2 \n",
    "    model.fit([tX, tXq], [tYBegin, tYEnd], epochs=10, batch_size=128, shuffle=True, validation_split=0.25, \\\n",
    "              callbacks=callbacks_list)\n",
    "    predictions = model.predict([vX, vXq], batch_size=128)\n",
    "    ansBegin = np.zeros((predictions[0].shape[0],), dtype=np.int32)\n",
    "    ansEnd = np.zeros((predictions[0].shape[0],),dtype=np.int32) \n",
    "    for i in range(predictions[0].shape[0]):\n",
    "        ansBegin[i] = predictions[0][i, :].argmax()\n",
    "        ansEnd[i] = predictions[1][i, :].argmax()\n",
    "    answers = {}\n",
    "    for i in range(len(validationQuestion_id)):\n",
    "        #print i\n",
    "        if ansBegin[i] >= len(validationContext[i]):\n",
    "            answers[validationQuestion_id[i]] = \"\"\n",
    "        elif ansEnd[i] >= len(validationContext[i]):\n",
    "            answers[validationQuestion_id[i]] = validationContextOriginal[i][validationToken2CharIdx[i][ansBegin[i]]:]\n",
    "        else:\n",
    "            answers[validationQuestion_id[i]] = validationContextOriginal[i][validationToken2CharIdx[i][ansBegin[i]]:validationToken2CharIdx[i][ansEnd[i]]+len(validationContext[i][ansEnd[i]])]\n",
    "    with io.open('predictedanswer.json', 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(answers, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximumcontext 844\n",
      "maximumquestion 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "cinput (InputLayer)             (None, 844)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "qinput (InputLayer)             (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 844, 300)     26538300    cinput[0][0]                     \n",
      "                                                                 qinput[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 300)          541200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 300)          541200      embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Merge)                 (None, 600)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 844)          507244      merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge2 (Merge)                  (None, 1444)         0           merge_1[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 844)          1219580     merge2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 29,347,524\n",
      "Trainable params: 29,347,524\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 65699 samples, validate on 21900 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(128, 150), b.shape=(150, 150), m=128, n=150, k=150\n\t [[Node: bidirectional_1/while/MatMul_7 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](bidirectional_1/while/Switch_2:1, bidirectional_1/while/MatMul_7/Enter)]]\n\t [[Node: metrics/acc_1/Mean/_267 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6611_metrics/acc_1/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'bidirectional_1/while/MatMul_7', defined at:\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 760, in _run_callback\n    ret = callback()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-263240bbee7e>\", line 1, in <module>\n    main()\n  File \"<ipython-input-17-406ea2b35040>\", line 20, in main\n    D = Bidirectional(LSTM(150, return_sequences=False))(cembedding)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\wrappers.py\", line 324, in __call__\n    return super(Bidirectional, self).__call__(inputs, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\wrappers.py\", line 384, in call\n    y = self.forward_layer.call(inputs, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 2151, in call\n    initial_state=initial_state)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 608, in call\n    input_length=timesteps)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2767, in rnn\n    swap_memory=True)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2816, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2640, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2590, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2753, in _step\n    tuple(constants))\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 599, in step\n    return self.cell.call(inputs, states, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 1947, in call\n    self.recurrent_kernel_o))\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1075, in dot\n    out = tf.matmul(x, y)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1891, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2436, in _mat_mul\n    name=name)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(128, 150), b.shape=(150, 150), m=128, n=150, k=150\n\t [[Node: bidirectional_1/while/MatMul_7 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](bidirectional_1/while/Switch_2:1, bidirectional_1/while/MatMul_7/Enter)]]\n\t [[Node: metrics/acc_1/Mean/_267 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6611_metrics/acc_1/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(128, 150), b.shape=(150, 150), m=128, n=150, k=150\n\t [[Node: bidirectional_1/while/MatMul_7 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](bidirectional_1/while/Switch_2:1, bidirectional_1/while/MatMul_7/Enter)]]\n\t [[Node: metrics/acc_1/Mean/_267 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6611_metrics/acc_1/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-406ea2b35040>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m#     checkpoint2 = EarlyStopping(monitor='val_dense_1_loss', min_delta=0.01, patience=3, verbose=0, mode='auto')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mcallbacks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#checkpoint2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtXq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtYBegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtYEnd\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvXq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mansBegin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(128, 150), b.shape=(150, 150), m=128, n=150, k=150\n\t [[Node: bidirectional_1/while/MatMul_7 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](bidirectional_1/while/Switch_2:1, bidirectional_1/while/MatMul_7/Enter)]]\n\t [[Node: metrics/acc_1/Mean/_267 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6611_metrics/acc_1/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'bidirectional_1/while/MatMul_7', defined at:\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 760, in _run_callback\n    ret = callback()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-263240bbee7e>\", line 1, in <module>\n    main()\n  File \"<ipython-input-17-406ea2b35040>\", line 20, in main\n    D = Bidirectional(LSTM(150, return_sequences=False))(cembedding)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\wrappers.py\", line 324, in __call__\n    return super(Bidirectional, self).__call__(inputs, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\wrappers.py\", line 384, in call\n    y = self.forward_layer.call(inputs, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 2151, in call\n    initial_state=initial_state)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 608, in call\n    input_length=timesteps)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2767, in rnn\n    swap_memory=True)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2816, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2640, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2590, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2753, in _step\n    tuple(constants))\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 599, in step\n    return self.cell.call(inputs, states, **kwargs)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 1947, in call\n    self.recurrent_kernel_o))\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1075, in dot\n    out = tf.matmul(x, y)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1891, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2436, in _mat_mul\n    name=name)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"c:\\users\\varun\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(128, 150), b.shape=(150, 150), m=128, n=150, k=150\n\t [[Node: bidirectional_1/while/MatMul_7 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](bidirectional_1/while/Switch_2:1, bidirectional_1/while/MatMul_7/Enter)]]\n\t [[Node: metrics/acc_1/Mean/_267 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6611_metrics/acc_1/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
